{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_forecasting import Baseline, DeepAR, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.data.examples import generate_ar_data\n",
    "from pytorch_forecasting.metrics import SMAPE, MultivariateNormalDistributionLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import setup_notebook\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "from grandexchange.preprocess import load_preprocessed_data\n",
    "\n",
    "# How many steps in the future to predict\n",
    "N_PREDICT_STEPS = 7\n",
    "\n",
    "# Number of steps for the first cross validation fold\n",
    "MIN_CV_SIZE = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = load_preprocessed_data(24)\n",
    "\n",
    "# For now take a sample of 5 items to test model\n",
    "# data = data[\n",
    "#     data[\"item_id\"].isin(\n",
    "#         data[\"item_id\"].sample(50, random_state=42)\n",
    "#     )\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Cross-validation\n",
    "\n",
    "Logic:\n",
    "- Forward rolling cross validation with MIN_CV_SIZE as the first fold\n",
    "- This initial training set will be the first fold and  will predict the next N_PREDICT_STEPS (test set) and store:\n",
    "    1. The predicted and actual\n",
    "    1. The number of days ahead the prediction was for\n",
    "- The new training set will be the previous fold plus the previous test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "steps = data.datetime.unique()\n",
    "cv_folds = {}\n",
    "\n",
    "fold = 0\n",
    "\n",
    "train_size = MIN_CV_SIZE\n",
    "train_fold = steps[:train_size]\n",
    "test_fold = steps[train_size:(train_size + N_PREDICT_STEPS)]\n",
    "\n",
    "while train_size + N_PREDICT_STEPS <= len(steps):\n",
    "    cv_folds[fold] = {\n",
    "        \"train\": train_fold,\n",
    "        \"test\": test_fold\n",
    "    }\n",
    "    fold += 1\n",
    "    train_size += N_PREDICT_STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Baseline model\n",
    "\n",
    "Predict the next days as the average of the previous 7 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_baseline_model():\n",
    "    fold_results = []\n",
    "\n",
    "    for fold in cv_folds.keys():\n",
    "        train = data[data[\"datetime\"].isin(cv_folds[fold][\"train\"][-7:])]\n",
    "        test = data[data[\"datetime\"].isin(cv_folds[fold][\"test\"])]\n",
    "        pred = train.groupby(\"item_id\")[\"price\"].mean().reset_index()\n",
    "\n",
    "        preds = pd.concat([\n",
    "            pred.assign(fold=x) for x in cv_folds.keys()\n",
    "        ], axis=0).rename({\"price\": \"predicted\"}, axis=1)\n",
    "\n",
    "    # Add all test datetimes\n",
    "    cv_results = pd.merge(\n",
    "        preds,\n",
    "        test[[\"item_id\", \"datetime\"]],\n",
    "        on=[\"item_id\"]\n",
    "    )\n",
    "    # Add actual price to each datetime and item_id\n",
    "    cv_results = pd.merge(\n",
    "        cv_results,\n",
    "        test[[\"item_id\", \"price\", \"datetime\"]].rename({\"price\": \"actual\"}, axis=1),\n",
    "        on=[\"item_id\", \"datetime\"]\n",
    "    )\n",
    "    # Add step\n",
    "    cv_results = cv_results.merge(\n",
    "        (\n",
    "            cv_results[[\"datetime\"]]\n",
    "            .drop_duplicates()\n",
    "            .reset_index(drop=True)\n",
    "            .reset_index()\n",
    "        ).rename({\"index\": \"step\"}, axis=1),\n",
    "        on=\"datetime\"\n",
    "    )\n",
    "    # cv_results[\"mape\"] = mean_absolute_percentage_error(cv_results[\"actual\"], cv_results[\"predicted\"])\n",
    "    cv_eval = cv_results.groupby([\"step\"]).apply(lambda x: mean_absolute_percentage_error(x[\"actual\"], x[\"predicted\"]))\n",
    "    return cv_results.reset_index(drop=True), cv_eval.reset_index(name=\"MAPE\")\n",
    "\n",
    "baseline_results = run_baseline_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(10, 6)})\n",
    "sns.lineplot(data=baseline_results[1], x=\"step\", y=\"MAPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DeepAR\n",
    "\n",
    "Multivariate deep learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_forecasting import Baseline, DeepAR, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from pytorch_forecasting.data.examples import generate_ar_data\n",
    "from pytorch_forecasting.metrics import SMAPE, MultivariateNormalDistributionLoss\n",
    "\n",
    "data_ar = (\n",
    "    data\n",
    "    .assign(price=data.groupby(\"item_id\")[\"price\"].apply(lambda x: (x - x.mean()) / x.std()))\n",
    "    .assign(time_idx=LabelEncoder().fit_transform(data[\"datetime\"]))\n",
    "    [[\"item_id\", \"price\", \"time_idx\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use the information from the last encode_length, to predict the max prediction_length\n",
    "max_encoder_length = 48\n",
    "max_prediction_length = 16\n",
    "\n",
    "training_cutoff = data_ar[\"time_idx\"].max() - max_prediction_length\n",
    "training = TimeSeriesDataSet(\n",
    "    data_ar[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"price\",\n",
    "    group_ids=[\"item_id\"],\n",
    "#     time_varying_unknown_reals=[\"margin\", \"volume\"],\n",
    "    time_varying_unknown_reals=[\"price\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    ")\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, data_ar, min_prediction_idx=training_cutoff + 1)\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Calculate baseline error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "baseline_predictions = Baseline().predict(val_dataloader)\n",
    "print(SMAPE()(baseline_predictions, actuals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Train network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "pl.seed_everything(42)\n",
    "import pytorch_forecasting as ptf\n",
    "\n",
    "trainer = pl.Trainer(gpus=0, gradient_clip_val=1e-1)\n",
    "net = DeepAR.from_dataset(\n",
    "    training, learning_rate=3e-2, hidden_size=16, rnn_layers=2, loss=MultivariateNormalDistributionLoss(rank=30)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    net,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    min_lr=1e-5,\n",
    "    max_lr=5e0,\n",
    "    early_stop_threshold=100,\n",
    ")\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "net.hparams.learning_rate = res.suggestion()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,\n",
    "    gradient_clip_val=0.01,\n",
    "    callbacks=[early_stop_callback],\n",
    "    limit_train_batches=50\n",
    ")\n",
    "\n",
    "\n",
    "net = DeepAR.from_dataset(\n",
    "    training,\n",
    "    learning_rate=res.suggestion(),\n",
    "    log_interval=10,\n",
    "    log_val_interval=1,\n",
    "    hidden_size=16,\n",
    "    rnn_layers=2,\n",
    "    loss=MultivariateNormalDistributionLoss(rank=30),\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    net,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_model = DeepAR.load_from_checkpoint(best_model_path)\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_model.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()\n",
    "\n",
    "raw_predictions, x = net.predict(val_dataloader, mode=\"raw\", return_x=True, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_item_name(item_id):\n",
    "    key = data[[\"item_id\", \"name\"]].drop_duplicates()\n",
    "    return key[key[\"item_id\"] == item_id][\"name\"].values\n",
    "\n",
    "series = validation.x_to_index(x)[\"item_id\"]\n",
    "for idx in range(300):  # plot 10 examples\n",
    "    print(idx)\n",
    "    best_model.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)\n",
    "    plt.suptitle(f\"Series: {get_item_name(series.iloc[idx])[0]}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Temporal Fusion Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ar = (\n",
    "    data\n",
    "    .assign(price=data.groupby(\"item_id\")[\"price\"].apply(lambda x: (x - x.mean()) / x.std()))\n",
    "    .assign(time_idx=LabelEncoder().fit_transform(data[\"datetime\"]))\n",
    "    .drop([\"datetime\", \"name\"], axis=1)\n",
    ")\n",
    "data_ar.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 14\n",
    "max_encoder_length = 30\n",
    "training_cutoff = data_ar['time_idx'].max() - max_prediction_length\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data_ar[lambda x: x['time_idx'] <= training_cutoff],\n",
    "    time_idx='time_idx',\n",
    "    target=\"price\",\n",
    "    group_ids=[\"item_id\"],\n",
    "    min_encoder_length=0,  \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    time_varying_known_reals=['time_idx'],\n",
    "    time_varying_unknown_reals=['price', \"margin\", \"volume\"],\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = TimeSeriesDataSet.from_dataset(training, data_ar, predict=True, stop_randomization=True)\n",
    "\n",
    "batch_size = 256\n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.metrics import RMSE\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "\n",
    "# configure network and trainer\n",
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    gpus=0,\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    # not meaningful for finding the learning rate but otherwise very important\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # most important hyperparameter apart from learning rate\n",
    "    # number of attention heads. Set to up to 4 for large datasets\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,  # between 0.1 and 0.3 are good values\n",
    "    hidden_continuous_size=8,  # set to <= hidden_size\n",
    "    output_size=1,  # 7 quantiles by default\n",
    "    loss=RMSE(),\n",
    "    # reduce learning rate if no improvement in validation loss after x epochs\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=0.1,\n",
    "    min_lr=1e-7,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-7, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  \n",
    "logger = TensorBoardLogger(\"lightning_logs\") \n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    gpus=0,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=30,  \n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.00724,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=1, \n",
    "    loss=RMSE(),\n",
    "    log_interval=10,  \n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "actuals = torch.cat([y[0] for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "(actuals - predictions).abs().mean()\n",
    "raw_predictions, x = best_tft.predict(val_dataloader, mode=\"raw\", return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(400):  # plot 10 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)\n",
    "    plt.title(f\"Series: {get_item_name(series.iloc[idx])[0]}\\n\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_name(item_id):\n",
    "    key = data[[\"item_id\", \"name\"]].drop_duplicates()\n",
    "    return key[key[\"item_id\"] == item_id][\"name\"].values\n",
    "\n",
    "series = validation.x_to_index(x)[\"item_id\"]\n",
    "for idx in range(300):  # plot 10 examples\n",
    "    print(idx)\n",
    "    best_model.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)\n",
    "    plt.suptitle(f\"Series: {get_item_name(series.iloc[idx])[0]}\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f3e7417fa867386be153df2f61b8901e04c2dcf9c270e2e4cdc45a16a9b3a706"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
